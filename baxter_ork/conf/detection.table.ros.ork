source1:
  type: RosKinect
  module: object_recognition_ros.io.source.ros_kinect
  parameters:
    rgb_frame_id: 'camera_rgb_optical_frame'
    depth_frame_id: 'camera_depth_optical_frame'
    depth_camera_info: /camera/depth_registered/camera_info
    depth_image_topic: /camera/depth_registered/image_raw
    rgb_camera_info: /camera/rgb/camera_info
    rgb_image_topic: /camera/rgb/image_color

sink1:
  type: TablePublisher
  module: 'object_recognition_tabletop'
  inputs: [source1]
sink2:
   type: Publisher
   module: object_recognition_ros.io.sink.publisher
   parameters:
      sensor_frame: /camera_rgb_optical_frame
      robot_frame: /world
      #  The DB parameters
      db_params: <object_recognition_core.boost.interface.ObjectDbParameters object at 0x1d34fc8>
      #  Determines if the topics will be latched.
      latched: True
      #  The ROS topic to use for the marker array.
      markers_topic: markers
      #  The ROS topic to use for the object meta info string
      object_ids_topic: object_ids
      #  The ROS topic to use for the pose array.
      pose_topic: poses
      #  Sets whether the point cloud clusters have to be published or not
      publish_clusters: True
      #  The ROS topic to use for the recognized object
      recognized_object_array_topic: recognized_object_array

pipeline1:
  type: TabletopTableDetector
  module: 'object_recognition_tabletop'
  inputs: [source1]
  outputs: [sink1, sink2]
  parameters:
    sensor_frame: /camera_rgb_optical_frame
    robot_frame: /world
    table_detector:
      min_table_size: 4000
      plane_threshold: 0.01
    vertical_frame_id: /world
    #clusterer:
    #  table_z_filter_max: 0.35
    #  table_z_filter_min: 0.025
